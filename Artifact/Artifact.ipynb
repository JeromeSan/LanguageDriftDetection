{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGCjq0of8uTC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from itertools import compress\n",
        "import random\n",
        "from random import sample\n",
        "import pickle\n",
        "import autosklearn.classification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_recall_fscore_support\n",
        "import glob\n",
        "from time import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skmultiflow.meta import AdaptiveRandomForestClassifier,LearnPPNSEClassifier\n",
        "from skmultiflow.drift_detection import EDDM, HDDM_A\n",
        "import math\n",
        "\n",
        "rng = np.random.default_rng(4711)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2zR_0on2Wf3"
      },
      "source": [
        "# Artifact\n",
        "\n",
        "\n",
        "1.   Artifact with Classifier\n",
        "2.   Pure Detector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pfn3zu52RNJ"
      },
      "outputs": [],
      "source": [
        "class Artifact():\n",
        "\n",
        "  def __init__(self, training_data, model, batch_size = 500):\n",
        "    self.eddm = EDDM()\n",
        "    self.hddm = HDDM_A()\n",
        "    self.training_data = training_data\n",
        "    self.model = model\n",
        "    self.training_size = len(training_data)\n",
        "    self.corpus_save_DataFrame = pd.DataFrame()\n",
        "    self.inner_PWI = None\n",
        "    self.general_corpus = None\n",
        "    self.data_buffer = training_data\n",
        "    self.batch_size = batch_size\n",
        "    self.batch_len = 0 \n",
        "    self.test_corpora = 2\n",
        "    self.change_detected = False\n",
        "\n",
        "    # vectorizer definition\n",
        "    german_stop_words_import = open (\"/LanguageDriftDetection/Data/Stopwords/stopwords-de.csv\", \n",
        "                                 \"r\", encoding=\"utf-8\")\n",
        "    german_stop_words = pd.read_csv(german_stop_words_import,\n",
        "                                names=['stopwords'])['stopwords'].to_list()\n",
        "    self.vectorizer = TfidfVectorizer(stop_words=german_stop_words, \n",
        "                             ngram_range=(1,2), max_features=3000)\n",
        "    self.vectorizer = self.vectorizer.fit(training_data.Text)\n",
        "    \n",
        "    self.set_training_corpus(training_data)\n",
        "\n",
        "\n",
        "  def predict(self, x):\n",
        "    if isinstance(x, str):\n",
        "      x = vectorizer.transform([x]).toarray()\n",
        "\n",
        "    return self.model.predict(x)[0]\n",
        "\n",
        "  def add_data(self, x, y):\n",
        "    input = pd.DataFrame(data={'Text':[x], 'Abgelehnt':[y]})\n",
        "    self.data_buffer = pd.concat([self.data_buffer, input], ignore_index=True)\n",
        "    self.batch_len += 1\n",
        "    y_pred = self.predict(x)\n",
        "    error = abs(y_pred - y)\n",
        "    adwinError = abs(abs(y_pred-y)-1)\n",
        "\n",
        "    self.eddm.add_element(error)\n",
        "    self.hddm.add_element(error)\n",
        "\n",
        "    if (self.change_detected == False) & ((self.eddm.detected_change() or self.hddm.detected_change())):\n",
        "      self.change_detected = True\n",
        "\n",
        "\n",
        "    if (self.batch_len >= self.batch_size) & (self.change_detected == True):\n",
        "      print('Refitting a batch of size ', self.batch_len)\n",
        "      #test attempt without WI\n",
        "      # training_size = len(self.training_data)\n",
        "      # self.training_data = pd.concat([self.training_data, self.data_buffer])[:-training_size]\n",
        "      # print(self.training_data)\n",
        "      self.set_training_corpus(self.data_buffer)\n",
        "      self.data_buffer = self.training_data\n",
        "      x_train = self.vectorizer.fit_transform(self.training_data.Text).toarray()\n",
        "      self.model.reset()\n",
        "      self.model.partial_fit(x_train, self.training_data.Abgelehnt, [0,1])\n",
        "      self.batch_len = 0\n",
        "      self.change_detected = False\n",
        "\n",
        "    \n",
        "  #  sets the general corpus for the WI and calculates Inner WI for thresholding\n",
        "  def set_training_corpus(self, corpus):\n",
        "    \n",
        "    # intital setting of base corpus and PWI\n",
        "    if self.corpus_save_DataFrame.empty:\n",
        "      self.corpus_save_DataFrame = corpus\n",
        "      self.test_corpora = int(len(self.corpus_save_DataFrame)/self.batch_size)\n",
        "      self.general_corpus = self.splitPivotText(corpus)\n",
        "      self.inner_PWI = self.calcPWI(corpus)\n",
        "\n",
        "\n",
        "    # updating base corpus with new training data\n",
        "    else:\n",
        "      if self.test_corpora <= 1:\n",
        "        self.test_corpora = 2 \n",
        "\n",
        "      # adding new Corpus to Base Corpus to create Baseline\n",
        "      # if the new data set is closer to the dictionary than the existing one no new data will be added to the training set to avoid overfitting\n",
        "      compare_PWI = self.calcPWI(pd.concat([self.corpus_save_DataFrame, corpus]))\n",
        "      if compare_PWI > self.inner_PWI:\n",
        "        self.inner_PWI = compare_PWI\n",
        "      samples = np.array_split(self.corpus_save_DataFrame.sample(frac=1), self.test_corpora)\n",
        "\n",
        "      for sample in samples:\n",
        "        sample_corpus = pd.concat([sample, corpus])\n",
        "        test_corpus = self.splitPivotText(sample_corpus)\n",
        "\n",
        "        test_PWI = self.calcPWI(sample)\n",
        "\n",
        "        if test_PWI < self.inner_PWI:\n",
        "          print('New General Corpus set. Inner PWI at: ', test_PWI)\n",
        "          self.general_corpus = test_corpus\n",
        "          self.training_data = sample.reset_index(drop=True)\n",
        "          self.inner_PWI = test_PWI\n",
        "\n",
        "  def calcPWI(self, corpus):\n",
        "    general_Corpus, specialized_Corpus = train_test_split(corpus, test_size=0.5, random_state=4711) \n",
        "    pwi_frame = self.calcPWIFrame(self.splitPivotText(general_Corpus), self.splitPivotText(specialized_Corpus))\n",
        "    pwi = pwi_frame.WI_Time.sum() / pwi_frame.WI_Time.count()\n",
        "\n",
        "    return pwi\n",
        "\n",
        "  def splitPivotText(self, df):\n",
        "    functionDF =  df.copy()\n",
        "    functionDF['Text'] = functionDF['Text'].astype(str)\n",
        "    functionDF['Text'] = functionDF['Text'].apply(lambda x: x.split())\n",
        "    splitFrame = pd.DataFrame(functionDF['Text'].to_list())\n",
        "    splitFrame['Label'] = functionDF.reset_index().Abgelehnt\n",
        "\n",
        "    # create one column with every word and their occurence with labels\n",
        "    vocab = splitFrame.melt(id_vars='Label').drop(columns=['variable'])\n",
        "    # turns the label column into two separate columns and aggregates them\n",
        "    vocab['Removed'] = np.where(vocab['Label'] == 1, 1,0)\n",
        "    vocab['not_Removed'] = np.where(vocab['Label'] == np.nan, 0,1)\n",
        "    vocab = vocab.drop(columns=['Label'])\n",
        "\n",
        "    vocab['Removed'] = vocab.groupby('value')['Removed'].transform('sum')\n",
        "    vocab['not_Removed'] = vocab.groupby('value')['not_Removed'].transform('sum')\n",
        "    returnDF = vocab.drop_duplicates(subset=['value'])\n",
        "\n",
        "    returnDF = returnDF[returnDF['value'].notnull()]\n",
        "\n",
        "    return returnDF\n",
        "\n",
        "\n",
        "\n",
        "  def calcPWIFrame(self, generalCorpus, specializedCorpus):\n",
        "\n",
        "    mergedPivot = specializedCorpus.set_index('value').join(generalCorpus.set_index('value'), how='left', lsuffix='_specialized', rsuffix='_corpus').fillna(0).reset_index()\n",
        "    ts_time = specializedCorpus.Removed.sum() + specializedCorpus.not_Removed.sum()\n",
        "    tg_time = generalCorpus.Removed.sum() + generalCorpus.Removed.sum()  \n",
        "\n",
        "\n",
        "    mergedPivot['WI_Time'] = mergedPivot.apply(lambda x: self.calculate_polarized_pwi(x['Removed_specialized'] + x['not_Removed_specialized'], x['Removed_corpus'] + x['not_Removed_corpus'], ts_time, tg_time),axis=1)\n",
        "    \n",
        "    return mergedPivot\n",
        "\n",
        " \n",
        " \n",
        "  def calculate_polarized_pwi(self, ws, wg, ts, tg):\n",
        "\n",
        "    if((ws > 0) & (wg > 0)):  \n",
        "      pwi = (ws/ts)/(wg/tg)\n",
        "\n",
        "    elif(ws > 0):\n",
        "      pwi = ws/ts\n",
        "\n",
        "    elif(wg > 0):\n",
        "      pwi = ws / (wg/tg)\n",
        "\n",
        "    else:\n",
        "      pwi = 0\n",
        "\n",
        "    if np.isnan(pwi):\n",
        "      pwi = 0\n",
        "\n",
        "    return pwi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detector"
      ],
      "metadata": {
        "id": "DIqc5kwz0OCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCpdo5VLYvCh"
      },
      "outputs": [],
      "source": [
        "class detectorEnsemble():\n",
        "\n",
        "  def __init__(self, training_data, model, batch_size = 500):\n",
        "    self.eddm = EDDM()\n",
        "    self.hddm = HDDM_A()\n",
        "    self.training_data = training_data\n",
        "    self.model = model\n",
        "    self.training_size = len(training_data)\n",
        "    self.corpus_save_DataFrame = pd.DataFrame()\n",
        "    self.inner_PWI = None\n",
        "    self.general_corpus = None\n",
        "    self.data_buffer = training_data\n",
        "    self.batch_size = batch_size\n",
        "    self.batch_len = 0 \n",
        "    self.test_corpora = 2\n",
        "    self.change_detected = False\n",
        "\n",
        "\n",
        "     # vectorizer definition\n",
        "    german_stop_words_import = open (\"/LanguageDriftDetection/Data/Stopwords/stopwords-de.csv\", \n",
        "                                 \"r\", encoding=\"utf-8\")\n",
        "    german_stop_words = pd.read_csv(german_stop_words_import,\n",
        "                                names=['stopwords'])['stopwords'].to_list()\n",
        "    self.vectorizer = TfidfVectorizer(stop_words=german_stop_words, \n",
        "                             ngram_range=(1,2), max_features=3000)\n",
        "    self.vectorizer = self.vectorizer.fit(training_data.Text)\n",
        "    \n",
        "    self.set_training_corpus(training_data)\n",
        "\n",
        "\n",
        "    def add_data(self, error):\n",
        "\n",
        "    input = pd.DataFrame(data={'Text':[x], 'Abgelehnt':[y]})\n",
        "    self.data_buffer = pd.concat([self.data_buffer, input], ignore_index=True)\n",
        "    self.batch_len += 1\n",
        "\n",
        "    self.eddm.add_element(self.error)\n",
        "    self.hddm.add_element(self.error)\n",
        "\n",
        "    if (self.change_detected == False) & ((self.eddm.detected_change() or self.hddm.detected_change())):\n",
        "      self.change_detected = True\n",
        "\n",
        "\n",
        "    if (self.batch_len >= self.batch_size) & (self.change_detected == True):\n",
        "      print('Refitting a batch of size ', self.batch_len)\n",
        "      self.set_training_corpus(self.data_buffer)\n",
        "      self.data_buffer = self.training_data\n",
        "      x_train = self.vectorizer.fit_transform(self.training_data.Text).toarray()\n",
        "      self.batch_len = 0\n",
        "      self.change_detected = False\n",
        "      \n",
        "      return True\n",
        "\n",
        "    else:\n",
        "      return False\n",
        "    \n",
        "     \n",
        "\n",
        "     def set_training_corpus(self, corpus):\n",
        "    \n",
        "    # intital setting of base corpus and PWI\n",
        "    if self.corpus_save_DataFrame.empty:\n",
        "      self.corpus_save_DataFrame = corpus\n",
        "      self.test_corpora = int(len(self.corpus_save_DataFrame)/self.batch_size)\n",
        "      self.general_corpus = self.splitPivotText(corpus)\n",
        "      self.inner_PWI = self.calcPWI(corpus)\n",
        "\n",
        "\n",
        "    # updating base corpus with new training data\n",
        "    else:\n",
        "      if self.test_corpora <= 1:\n",
        "        self.test_corpora = 2 \n",
        "\n",
        "      # adding new Corpus to Base Corpus to create Baseline\n",
        "      # if the new data set is closer to the dictionary than the existing one no new data will be added to the training set to avoid overfitting\n",
        "      compare_PWI = self.calcPWI(pd.concat([self.corpus_save_DataFrame, corpus]))\n",
        "      if compare_PWI > self.inner_PWI:\n",
        "        self.inner_PWI = compare_PWI\n",
        "      samples = np.array_split(self.corpus_save_DataFrame.sample(frac=1), self.test_corpora)\n",
        "\n",
        "      for sample in samples:\n",
        "        sample_corpus = pd.concat([sample, corpus])\n",
        "        test_corpus = self.splitPivotText(sample_corpus)\n",
        "\n",
        "        test_PWI = self.calcPWI(sample)\n",
        "\n",
        "        if test_PWI < self.inner_PWI:\n",
        "          print('New General Corpus set. Inner PWI at: ', test_PWI)\n",
        "          self.general_corpus = test_corpus\n",
        "          self.training_data = sample.reset_index(drop=True)\n",
        "          self.inner_PWI = test_PWI\n",
        "\n",
        "  def calcPWI(self, corpus):\n",
        "    general_Corpus, specialized_Corpus = train_test_split(corpus, test_size=0.5, random_state=4711) \n",
        "    pwi_frame = self.calcPWIFrame(self.splitPivotText(general_Corpus), self.splitPivotText(specialized_Corpus))\n",
        "    pwi = pwi_frame.WI_Time.sum() / pwi_frame.WI_Time.count()\n",
        "\n",
        "    return pwi\n",
        "\n",
        "  def splitPivotText(self, df):\n",
        "    functionDF =  df.copy()\n",
        "    functionDF['Text'] = functionDF['Text'].astype(str)\n",
        "    functionDF['Text'] = functionDF['Text'].apply(lambda x: x.split())\n",
        "    splitFrame = pd.DataFrame(functionDF['Text'].to_list())\n",
        "    splitFrame['Label'] = functionDF.reset_index().Abgelehnt\n",
        "\n",
        "    # create one column with every word and their occurence with labels\n",
        "    vocab = splitFrame.melt(id_vars='Label').drop(columns=['variable'])\n",
        "    # turns the label column into two separate columns and aggregates them\n",
        "    vocab['Removed'] = np.where(vocab['Label'] == 1, 1,0)\n",
        "    vocab['not_Removed'] = np.where(vocab['Label'] == np.nan, 0,1)\n",
        "    vocab = vocab.drop(columns=['Label'])\n",
        "\n",
        "    vocab['Removed'] = vocab.groupby('value')['Removed'].transform('sum')\n",
        "    vocab['not_Removed'] = vocab.groupby('value')['not_Removed'].transform('sum')\n",
        "    returnDF = vocab.drop_duplicates(subset=['value'])\n",
        "\n",
        "    returnDF = returnDF[returnDF['value'].notnull()]\n",
        "\n",
        "    return returnDF\n",
        "\n",
        "\n",
        "\n",
        "  def calcPWIFrame(self, generalCorpus, specializedCorpus):\n",
        "\n",
        "    mergedPivot = specializedCorpus.set_index('value').join(generalCorpus.set_index('value'), how='left', lsuffix='_specialized', rsuffix='_corpus').fillna(0).reset_index()\n",
        "    ts_time = specializedCorpus.Removed.sum() + specializedCorpus.not_Removed.sum()\n",
        "    tg_time = generalCorpus.Removed.sum() + generalCorpus.Removed.sum()  \n",
        "\n",
        "\n",
        "    mergedPivot['WI_Time'] = mergedPivot.apply(lambda x: self.calculate_polarized_pwi(x['Removed_specialized'] + x['not_Removed_specialized'], x['Removed_corpus'] + x['not_Removed_corpus'], ts_time, tg_time),axis=1)\n",
        "    \n",
        "    return mergedPivot\n",
        "\n",
        " \n",
        " \n",
        "  def calculate_polarized_pwi(self, ws, wg, ts, tg):\n",
        "\n",
        "    if((ws > 0) & (wg > 0)):  \n",
        "      pwi = (ws/ts)/(wg/tg)\n",
        "\n",
        "    elif(ws > 0):\n",
        "      pwi = ws/ts\n",
        "\n",
        "    elif(wg > 0):\n",
        "      pwi = ws / (wg/tg)\n",
        "\n",
        "    else:\n",
        "      pwi = 0\n",
        "\n",
        "    if np.isnan(pwi):\n",
        "      pwi = 0\n",
        "\n",
        "    return pwi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "7cbMeBEcRjZz",
        "y_nYzFCqIBsI"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}